ğŸ›¡ï¸ Online Payment Fraud Detection using Machine Learning

ğŸ“Œ Project Overview

This project focuses on building a machine learning model to detect fraudulent online payment transactions. With the rise of digital transactions, fraud detection is crucial for ensuring security and reducing financial risks.

Using Python and popular ML libraries, I analyzed the dataset, performed feature engineering, and trained multiple classification models (Logistic Regression, Random Forest, XGBoost) to identify fraudulent behavior.

ğŸ“Š Dataset

The dataset contains transactional details such as:

step â±ï¸ (time step)

type ğŸ’³ (transaction type: CASH_OUT, TRANSFER, etc.)

amount ğŸ’° (transaction amount)

nameOrig, nameDest ğŸ‘¥ (sender and receiver IDs)

oldbalanceOrg, newbalanceOrig, oldbalanceDest, newbalanceDest

isFraud ğŸš¨ (target: 1 = Fraud, 0 = Legit)

ğŸ› ï¸ Tech Stack

Programming Language: Python ğŸ

Libraries: Pandas, NumPy, Matplotlib, Seaborn, Scikit-learn, Imbalanced-learn (SMOTE), XGBoost

ML Models: Logistic Regression, Random Forest, XGBoost

ğŸ“ˆ Workflow

Data Exploration & Visualization ğŸ“Š

Distribution of transaction types & amounts

Fraud vs. non-fraud imbalance check

Data Preprocessing ğŸ”„

Encoding categorical variables

Dropping irrelevant columns

Handling class imbalance with SMOTE

Model Training ğŸ¤–

Logistic Regression

Random Forest

XGBoost

Evaluation Metrics ğŸ“‘

Confusion Matrix

Precision, Recall, F1-score

ROC-AUC Curve

ğŸ“ˆ Understanding ROC & AUC
ğŸ”¹ ROC (Receiver Operating Characteristic) Curve

The ROC curve is a plot that shows how well a classification model distinguishes between classes (Fraud ğŸš¨ vs. Non-Fraud âœ…).

X-axis: False Positive Rate (FPR) â†’ how many legitimate transactions were incorrectly flagged as fraud.

Y-axis: True Positive Rate (TPR / Recall) â†’ how many frauds were correctly detected.

A good modelâ€™s curve hugs the top-left corner of the graph.

ğŸ”¹ AUC (Area Under the Curve)

The AUC is a single number that summarizes the ROC curve.

AUC = 0.5 â†’ Model is no better than random guessing ğŸ²

AUC = 1.0 â†’ Perfect classifier ğŸ¯

Closer to 1 â†’ Better fraud detection model

âœ… Why It Matters in Fraud Detection

Fraud detection datasets are usually imbalanced (frauds are rare).

Accuracy can be misleading (e.g., predicting "Not Fraud" for all transactions gives 99% accuracy).

ROC-AUC provides a much better measure of whether the model can actually distinguish fraud from legitimate transactions.

In this project, the Random Forest & XGBoost models achieved an AUC of 0.97, meaning they can correctly rank fraud vs. non-fraud transactions 97% of the time.

âœ… Results

Achieved ROC-AUC = 0.97

Fraud detection Recall improved to ~83% (catching more fraud cases)

F1-score = 0.85 balancing precision & recall

Reduced false positives by ~15% compared to baseline logistic regression



## ğŸ“‚ Dataset
The dataset (450MB) is too large to host on GitHub.  
â¡ï¸ Download it here: [Google Drive Link](https://drive.google.com/file/d/1WCN9M6B3zDR6gePhrR2omSEPR8LzTJpe/view?usp=drive_link) 
